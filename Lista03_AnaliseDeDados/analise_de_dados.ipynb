{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94270a21",
   "metadata": {},
   "source": [
    "# CK0223 - Mineração de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3312b2cd",
   "metadata": {},
   "source": [
    "## Lista 03 - Análise de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55540cb3",
   "metadata": {},
   "source": [
    "### Dados do discente:\n",
    "**Nome**: Luiza Esther Martins Pessoa\n",
    "**Matrícula**: 555516\n",
    "\n",
    "### Vídeo Youtube:\n",
    "[Mineração de Dados: Lista 03 - Análise de Dados (Explicando o código)]()\n",
    "\n",
    "### GitHub:\n",
    "[EstherMart - Análise de Dados](https://github.com/EstherMart/Data-Mining/blob/main/Lista02_ExtracaoDeDados/extracao.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cd9b6",
   "metadata": {},
   "source": [
    "### **(a)** Ler o dataset *fakeTelegram.BR_2022.csv*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5397f4ab",
   "metadata": {},
   "source": [
    "Como realizado na [Lista 01 - Tratamento de Dados](https://github.com/EstherMart/Data-Mining/blob/main/Lista01_TratamentoDeDados/tratamento.ipynb), ler o dataset é o primeiro passo para iniciarmos a extraçãa, manipulação e tratamento dos dados. \n",
    "\n",
    "Para fazer isso, começamos importando as bibliotecas necessárias para leitura (`pandas`) e para download local da base de dados (`gdown`). Além disso, vale ressaltar que existem outras formas de realizar o upload para o repositório local, mas decidi seguir a lógica de puxar e realizar o download de base utilizando apenas o link disponibilizado pelo professor.\n",
    "\n",
    "**Importante**: Alguns trechos de código serão reutilizados, visto que são as mesmas exigências em ambas as listas e tal solicitação já foi resolvida anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAÇÃO DE BIBLIOTECAS\n",
    "import gdown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0040aa",
   "metadata": {},
   "source": [
    "Fazendo o download do dataset para o repositório local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886133cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "origem_url = 'https://drive.google.com/file/d/1c_hLzk85pYw-huHSnFYZM_gn-dUsYRDm/view'\n",
    "\n",
    "# O ID do arquivo (necessário para fazer o download direto) está entre os últimos elementos da URL.\n",
    "# Fazemos um split na URL usando '/' como separador e pegamos o penúltimo elemento da lista.\n",
    "# Isso funciona porque a estrutura da URL é:\n",
    "# https://drive.google.com/file/d/ID_DO_ARQUIVO/view\n",
    "# E ao aplicar url.split('/'), o resultado será:\n",
    "# ['https:', '', 'drive.google.com', 'file', 'd', 'ID_DO_ARQUIVO', 'view?...']\n",
    "# Portanto, o ID está na posição -2 (penúltima).\n",
    "\n",
    "file_id = origem_url.split('/')[-2]\n",
    "\n",
    "# URL do arquivo no formato aceito pelo gdown\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "# Nome local do arquivo que será baixado\n",
    "output = 'fakeTelegram.BR_2022.csv'\n",
    "\n",
    "# Baixando o arquivo com gdown\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8568f39e",
   "metadata": {},
   "source": [
    "Leitura do dataset utilizando o método `.read_csv()`, pois permite carregar dados estruturados a partir de arquivos *CSV* para um DataFrame do pandas, sendo uma estrutura tabular extremamente versátil para análise de dados e visualmente intuitiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial = pd.read_csv(\"fakeTelegram.BR_2022.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424d8d9",
   "metadata": {},
   "source": [
    "Os códigos abaixo foram implementados apenas para fins de **visualização e verificação inicial dos dados carregados**, com o objetivo de:\n",
    "- Obter uma compreensão rápida da dimensão do dataset\n",
    "- Identificar os nomes das colunas disponíveis\n",
    "- Realizar uma primeira checagem da integridade básica da estrutura de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d158bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA MELHOR VISUALIZAÇÃO\n",
    "print(\"Número de linhas:\", df_inicial.shape[0])\n",
    "print(\"Número de colunas:\", df_inicial.shape[1])\n",
    "print(\"\\nColunas disponíveis:\")\n",
    "for i, col in enumerate(df_inicial.columns, 1):\n",
    "    print(f\"{i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ec9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZANDO AMOSTRAS INICIAIS E FINAIS DO DATASET\n",
    "display(df_inicial.head().style.set_caption(\"Primeiros Registros\").set_properties(**{\n",
    "    'background-color': '#f8f9fa',\n",
    "    'border': '1px solid #dee2e6',\n",
    "    'color': '#212529',\n",
    "    'max-width': '300px',\n",
    "    'overflow': 'hidden',\n",
    "    'text-overflow': 'ellipsis',\n",
    "    'white-space': 'nowrap'\n",
    "}))\n",
    "\n",
    "display(df_inicial.tail().style.set_caption(\"Últimos Registros\").set_properties(**{\n",
    "    'background-color': '#f8f9fa',\n",
    "    'border': '1px solid #dee2e6',\n",
    "    'color': '#212529',\n",
    "    'max-width': '300px',\n",
    "    'overflow': 'hidden',\n",
    "    'text-overflow': 'ellipsis',\n",
    "    'white-space': 'nowrap'\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24236d3",
   "metadata": {},
   "source": [
    "### **(b)** Remova os trava-zaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b745420",
   "metadata": {},
   "source": [
    "Este item será resolvido através da manipulação da coluna `trava_zap`, previamente identificada na análise inicial. A estratégia adotada considera o tamanho do dataset (557.586 registros) e segue um fluxo estruturado:\n",
    "\n",
    "1. **Análise Preliminar**\n",
    "    - Verificação do tipo de dado (dtype) e valores únicos na coluna `trava_zap`\n",
    "    - Confirmação da proporção True/False/NaN (se aplicável)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISE PRELIMINAR\n",
    "# Verificação do tipo e valores únicos\n",
    "print(f\"Tipo de dado: {df_inicial['trava_zap'].dtype}\")\n",
    "trava_dist = df_inicial['trava_zap'].value_counts(dropna=False)\n",
    "display(trava_dist.to_frame().style.set_caption(\"Contagem de valores distintos em trava_zap\"))\n",
    "\n",
    "# Verificação de linhas que devem ser removidas\n",
    "print(f\"Registros com trava_zap que precisam ser removidos: {trava_dist.get(True, 0):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7df72ab",
   "metadata": {},
   "source": [
    "2. **Processamento por Chunks**\n",
    "    - Divisão do dataset em blocos de 1.000 linhas para:\n",
    "        - Otimização de memória\n",
    "        - Facilidade de debug\n",
    "        - Monitoramento do progresso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed549dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6520cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros do chunk -- 1000 linhas \n",
    "CHUNK_SIZE = 1000\n",
    "total_chunks = (len(df_inicial) // CHUNK_SIZE) + 1\n",
    "df_sem_trava_zap = pd.DataFrame() # Criação de DataFrame Seguro para controle de variáveis.\n",
    "\n",
    "print(f\"\\033[1mProcessando {total_chunks} chunks de {CHUNK_SIZE} registros cada:\\033[0m\")\n",
    "\n",
    "for i, chunk in enumerate(np.array_split(df_inicial, total_chunks)):\n",
    "    # Filtro principal -- linhas que possuem trava zap, isto é, trava_zap = True\n",
    "    chunk_filtrado = chunk[chunk['trava_zap'] != True]\n",
    "    \n",
    "    # Concatenção segura -- recebendo apenas os blocos onde trava_zap = False\n",
    "    df_sem_trava_zap = pd.concat([df_sem_trava_zap, chunk_filtrado], ignore_index=True)\n",
    "    \n",
    "    # Log de progresso para controle da análise\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"Processado chunk {i+1}/{total_chunks} | Registros retidos: {len(df_sem_trava_zap):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relatório final\n",
    "print(\"\\033[1mRelatório de Remoção:\\033[0m\")\n",
    "print(f\"• Registros originais: {len(df_inicial):,}\")\n",
    "print(f\"• Registros removidos: {trava_dist.get(True, 0):,}\")\n",
    "print(f\"• Registros restantes: {len(df_sem_trava_zap):,}\") \n",
    "\n",
    "# Visualização dos dados que foram removidos\n",
    "if trava_dist.get(True, 0) > 0:\n",
    "    display(\n",
    "        df_inicial[df_inicial['trava_zap'] == True].head(16)  \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
